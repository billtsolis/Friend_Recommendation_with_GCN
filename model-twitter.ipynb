{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70716bc1-9511-41da-90af-d9c273b5d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.nn as gnn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.nn import GCNConv \n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import roc_auc_score, ndcg_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61707cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(TwitterDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['twitter_combined.txt', 'twitter_features.csv']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['twitter_data.pt']\n",
    "    \n",
    "    def download(self):\n",
    "        # In this case, you're not downloading any data,\n",
    "        # but you could add code to download your data here if it's not local\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # Load the graph data\n",
    "        edges = pd.read_csv(self.raw_paths[0], delimiter=' ', header=None)\n",
    "        features = pd.read_csv(self.raw_paths[1],index_col=0)\n",
    "\n",
    "        #encode features\n",
    "        words_list = features['features'].str.split(' ')\n",
    "        flattened_list = [word for sublist in words_list.dropna() for word in sublist]\n",
    "        # Convert the list into a set to find all unique words\n",
    "        unique_words = set(flattened_list)\n",
    "        unique_words=[x for x in unique_words]\n",
    "        #         unique_words.append('')\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        targets = le.fit_transform(unique_words)\n",
    "        encoded = dict(zip(unique_words, targets))\n",
    "\n",
    "        # Create the graph\n",
    "        edge_index = torch.tensor(edges.values, dtype=torch.long).t().contiguous()\n",
    "        fdict={}\n",
    "        for index, row in features.iterrows():\n",
    "            if pd.isna(row['features']) or row['features'] == '':\n",
    "                fdict[index]=tuple([])\n",
    "            else:\n",
    "                words= row['features']\n",
    "                wlist=words.split(\" \")\n",
    "                fdict[index] = tuple([encoded[key] for key in wlist])\n",
    "        max_length = max(len(value) for value in fdict.values())\n",
    "\n",
    "        padded_fdict = {key: value + (0,)* (max_length - len(value))  for key, value in fdict.items()}\n",
    "\n",
    "        x = torch.tensor(list(padded_fdict.values()),dtype=torch.long)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "        data.node_index = list(fdict.keys())\n",
    "\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "        \n",
    "        \n",
    "class GPlusDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(GPlusDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['gplus_combined.txt', 'gplus_features.txt']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['gplus_data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        # In this case, you're not downloading any data,\n",
    "        # but you could add code to download your data here if it's not local\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # Load the graph data\n",
    "        edges = pd.read_csv(self.raw_paths[0], delimiter=' ', header=None)\n",
    "        features = pd.read_csv(self.raw_paths[1], delimiter=' ', header=None)\n",
    "\n",
    "        # Create the graph\n",
    "        edge_index = torch.tensor(edges.values, dtype=torch.long).t().contiguous()\n",
    "        x = torch.tensor(features.values, dtype=torch.float)\n",
    "    \n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59022584",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TwitterDataset(root='.')\n",
    "# dataset\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85be3285-acd5-4474-a38c-0e59aaed5847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacebookDataset(InMemoryDataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(FacebookDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['facebook_combined.txt', 'facebook_features.txt']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data.pt']\n",
    "\n",
    "    def download(self):\n",
    "        # In this case, you're not downloading any data,\n",
    "        # but you could add code to download your data here if it's not local\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # Load the graph data\n",
    "        edges = pd.read_csv(self.raw_paths[0], delimiter=' ', header=None)\n",
    "        features = pd.read_csv(self.raw_paths[1], delimiter=' ', header=None)\n",
    "\n",
    "        # Create the graph\n",
    "        edge_index = torch.tensor(edges.values, dtype=torch.long).t().contiguous()\n",
    "        x = torch.tensor(features.values, dtype=torch.float)\n",
    "    \n",
    "        data = Data(x=x, edge_index=edge_index)\n",
    "\n",
    "        data, slices = self.collate([data])\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67b12142-a9ed-4579-9803-2e42be884e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = FacebookDataset(root='.')\n",
    "# data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ec53868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[81306, 194], edge_index=[2, 2420766], node_index=[81306])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd2f673c-3f99-4792-bcdd-8b034f8548f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Train Positives\n",
    "# Define the percentage of edges to remove\n",
    "test_percentage = 0.3\n",
    "\n",
    "# Calculate the number of edges to remove\n",
    "num_edges_to_remove = int(data.edge_index.shape[1] * test_percentage)\n",
    "\n",
    "# Shuffle the edges\n",
    "edge_indices = np.arange(data.edge_index.shape[1])\n",
    "np.random.shuffle(edge_indices)\n",
    "\n",
    "# Select the edges to keep\n",
    "edges_to_keep = edge_indices[num_edges_to_remove:]\n",
    "\n",
    "# Create a new graph with only the edges to keep(Train positives)\n",
    "data_prime = Data(x=data.x, edge_index=data.edge_index[:, edges_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d33966b-7563-47fe-99f1-a7ec50079019",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train pos \n",
    "train_pos=data_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e7311e5-3c2a-4052-ae32-484a27280ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST POSITIVES\n",
    "# Select the edges to remove (these will be your test positives)\n",
    "edges_to_remove = edge_indices[:num_edges_to_remove]\n",
    "\n",
    "# Create a new graph with only the edges to remove (Test positives)\n",
    "test_pos = Data(x=data.x, edge_index=data.edge_index[:, edges_to_remove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5e0b2fb-2730-4ead-b56a-e77c5b0fad06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Train negatives\n",
    "positives_edges=data_prime.edge_index\n",
    "# Number of negative samples to generate\n",
    "num_neg_samples = data_prime.edge_index.size(1)\n",
    "# Generate negative samples\n",
    "negative_edge_index = negative_sampling(edge_index=data.edge_index, num_nodes=data.num_nodes, num_neg_samples=num_neg_samples)\n",
    "#Train negatives\n",
    "train_neg = Data(x=data.x, edge_index=negative_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c90983f-7f26-4171-8e7a-a0505ea191cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST negatives\n",
    "# Number of negative samples to generate for testing\n",
    "num_test_neg_samples = test_pos.edge_index.size(1)\n",
    "\n",
    "# Generate negative samples for testing\n",
    "test_negative_edge_index = negative_sampling(edge_index=data.edge_index, num_nodes=data.num_nodes, num_neg_samples=num_test_neg_samples)\n",
    "\n",
    "# Convert tensors to lists of tuples\n",
    "train_pos_edges = [tuple(edge) for edge in train_pos.edge_index.t().tolist()]\n",
    "test_pos_edges = [tuple(edge) for edge in test_pos.edge_index.t().tolist()]\n",
    "test_negative_edges = [tuple(edge) for edge in test_negative_edge_index.t().tolist()]\n",
    "\n",
    "#splitted box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6926dd9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "726229"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pos_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24816b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1694537"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_pos_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2448240d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "726229"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_negative_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "697b7001",
   "metadata": {},
   "outputs": [],
   "source": [
    "qqqqa=train_pos_edges+test_pos_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c735a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_neg_edge_index = []\n",
    "with tqdm(total=len(test_negative_edges), position=0, leave=True) as pbar:\n",
    "    for edge in test_negative_edges:\n",
    "        if edge not in qqqqa:\n",
    "            test_neg_edge_index.append(edge)\n",
    "        pbar.update()\n",
    "\n",
    "# Create the test_neg set\n",
    "test_neg = Data(x=data.x, edge_index=torch.tensor(test_neg_edge_index).t().contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb27f018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2421/2421 [9:39:06<00:00, 14.35s/it]\n"
     ]
    }
   ],
   "source": [
    "#invers\n",
    "cnter = []\n",
    "with tqdm(total=int(len(qqqqa)/1000)+1, position=0, leave=True) as pbar:\n",
    "    for batch in range(int(len(qqqqa)/1000)+1):\n",
    "        if(batch+1000>len(qqqqa)):\n",
    "            xd=qqqqa[batch*1000:]\n",
    "        else:\n",
    "            xd=qqqqa[batch*1000:batch*1000+1000]\n",
    "        for edge in xd:\n",
    "            if edge in test_negative_edges:\n",
    "                cnter.append(edge)\n",
    "        pbar.update()\n",
    "\n",
    "# Create the test_neg set\n",
    "test_neg = Data(x=data.x, edge_index=torch.tensor(test_negative_edges).t().contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5174d0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_neg.validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0348573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "# qqqqa=train_pos_edges+test_pos_edges\n",
    "# # Function to check if an edge meets the condition\n",
    "# def check_edge(edge):\n",
    "#     if edge not in qqqqa:\n",
    "#         return edge\n",
    "#     return None\n",
    "\n",
    "# # Initialize list to store results\n",
    "# test_neg_edge_index = []\n",
    "\n",
    "# # Submit tasks to ThreadPoolExecutor\n",
    "# with tqdm(total=len(test_negative_edges), position=0, leave=True) as pbar:\n",
    "#     with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#         futures = [executor.submit(check_edge, edge) for edge in test_negative_edges]\n",
    "#         pbar.update()\n",
    "\n",
    "#     # Iterate over completed futures\n",
    "#         for future in concurrent.futures.as_completed(futures):\n",
    "#             result = future.result()\n",
    "#             if result is not None:\n",
    "#                 test_neg_edge_index.append(result)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af86a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "\n",
    "# qqqqa=train_pos_edges+test_pos_edges\n",
    "# # Function to check if an edge meets the condition\n",
    "# def check_edge(edge):\n",
    "#     if edge not in qqqqa:\n",
    "#         return edge\n",
    "#     return None\n",
    "\n",
    "# # Initialize list to store results\n",
    "# test_neg_edge_index = []\n",
    "\n",
    "# # Submit tasks to ThreadPoolExecutor using map function\n",
    "# with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "#     results = executor.map(check_edge, test_negative_edges)\n",
    "\n",
    "#     # Iterate over results and update progress bar\n",
    "#     with tqdm(total=len(test_negative_edges), position=0, leave=True) as pbar:\n",
    "#         for result in results:\n",
    "#             if result is not None:\n",
    "#                 test_neg_edge_index.append(result)\n",
    "#             pbar.update()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c23d302",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4550db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that these negative samples are not in the train_pos or test_pos sets\n",
    "test_neg_edge_index = [edge for edge in test_negative_edges if edge not in train_pos_edges and edge not in test_pos_edges]\n",
    "\n",
    "# Create the test_neg set\n",
    "test_neg = Data(x=data.x, edge_index=torch.tensor(test_neg_edge_index).t().contiguous())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9addb61-caf6-4572-98ad-9da3d37afbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_size):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_size)\n",
    "        self.conv2 = GCNConv(hidden_size, hidden_size)\n",
    "\n",
    "    def forward(self, edge_index, x):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2426c759",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncatedSVDModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, output_size):\n",
    "        super(TruncatedSVDModel, self).__init__()\n",
    "        self.svd = TruncatedSVD(n_components=output_size)\n",
    "        self.fc = torch.nn.Linear(output_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_svd = self.svd.fit_transform(x.numpy())\n",
    "        x_svd = torch.tensor(x_svd, dtype=torch.float)\n",
    "        x_out = self.fc(x_svd)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d975d5df-22fc-43db-a010-09cde635aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotPredictor(torch.nn.Module):\n",
    "    def forward(self, edge_index, h):\n",
    "        # Get the embeddings of the source nodes and destination nodes\n",
    "        source_node_embeddings = h[edge_index[0]]\n",
    "        destination_node_embeddings = h[edge_index[1]]\n",
    "\n",
    "        # Compute the dot product (score) between source and destination node embeddings\n",
    "        scores = (source_node_embeddings * destination_node_embeddings).sum(dim=-1)\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c62a5c93-c91f-4c9d-8c59-2c227b36767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(model_name='GCN', hidden_size=64, epoch=100):\n",
    "    \n",
    "    def compute_loss(pos_score, neg_score):  # computes the loss based on binary cross entropy\n",
    "        scores = torch.cat([pos_score, neg_score])\n",
    "        labels = torch.cat([torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])])\n",
    "        return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "    def compute_auc(pos_score, neg_score):  # computes AUC (Area-Under-Curve) score\n",
    "        scores = torch.cat([pos_score, neg_score]).numpy()\n",
    "        labels = torch.cat(\n",
    "            [torch.ones(pos_score.shape[0]), torch.zeros(neg_score.shape[0])]).numpy()\n",
    "        return roc_auc_score(labels, scores)\n",
    "       \n",
    "    # hidden_size is the size of the hidden layer in the neural net\n",
    "    if model_name == 'GCN':\n",
    "        model = GCN(data_prime.num_features, hidden_size)\n",
    "    elif model_name == 'TruncatedSVD':\n",
    "        model = TruncatedSVDModel(data_prime.num_features, hidden_size)\n",
    "        \n",
    "    pred = DotPredictor()\n",
    "    optimizer = torch.optim.SGD(itertools.chain(model.parameters(), pred.parameters()), lr=0.01, momentum=0.9)\n",
    "    # Use a learning rate scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=500, gamma=0.5)\n",
    " \n",
    "\n",
    "        # ----------- training -------------------------------- #\n",
    "    train_g = data_prime\n",
    "    for e in range(epoch):\n",
    "        if model_name == 'GCN':\n",
    "            h = model(train_g.edge_index, train_g.x)\n",
    "        elif model_name == 'TruncatedSVD':\n",
    "            h = model(train_g.x)  # get node embeddings\n",
    "\n",
    "        # forward    \n",
    "        pos_score = pred(train_pos.edge_index, h)\n",
    "        neg_score = pred(train_neg.edge_index, h)\n",
    "        loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if e % 10 == 0:\n",
    "            print('In epoch {}, loss: {}'.format(e, loss))\n",
    "\n",
    "    # ----------- test and check results ---------------- #\n",
    "    with torch.no_grad():\n",
    "        pos_score = pred(test_pos.edge_index, h)\n",
    "        neg_score = pred(test_neg.edge_index, h)\n",
    "        auc=compute_auc(pos_score, neg_score)\n",
    "        print('AUC', auc)  \n",
    "               \n",
    "    # Print model's state_dict\n",
    "    print(\"Model's state_dict:\")\n",
    "    for param_tensor in model.state_dict():\n",
    "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "    # Print optimizer's state_dict\n",
    "    print(\"Optimizer's state_dict:\")\n",
    "    for var_name in optimizer.state_dict():\n",
    "        print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "\n",
    "    # Print scheduler's state_dict\n",
    "    print(\"scheduler's state_dict:\")\n",
    "    for var_name in scheduler.state_dict():\n",
    "        print(var_name, \"\\t\", scheduler.state_dict()[var_name])\n",
    "    \n",
    "    torch.save({\n",
    "            'epoch': epoch,        \n",
    "            'epoch_rem': e,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'loss': loss,\n",
    "            \n",
    "            }, './torch_model/model_'+model_name+'_'+str(epoch)+'_'+'twitter'+'.pt')\n",
    "        \n",
    "    return h  # return node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "426437d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rec(h, user_id=0, k=10):\n",
    "    # `h` represents the node embeddings, with shape [num_nodes, hidden_size]\n",
    "\n",
    "    # generate a graph with (num_nodes - num_friends_of_user) edges\n",
    "    # one end of the edge is user_id\n",
    "    # the other end is a user that's NOT friends with user_id\n",
    "    user_friends = set()\n",
    "    user_neg_u, user_neg_v = [], []\n",
    "\n",
    "    for n1, n2 in data.edge_index.t().tolist():   # get all friends of user_id\n",
    "        if int(n1) == user_id:\n",
    "            user_friends.add(int(n2))\n",
    "        if int(n2) == user_id:\n",
    "            user_friends.add(int(n1))\n",
    "\n",
    "    num_nodes=data.x.shape[0]\n",
    "    for i in range(num_nodes):  # generate \"negative edges\" for user_id\n",
    "        if i != user_id and i not in user_friends:\n",
    "            user_neg_u.append(user_id)\n",
    "            user_neg_v.append(i)\n",
    "\n",
    "    user_g = Data(x=data.x,edge_index=torch.tensor([user_neg_u, user_neg_v] ))\n",
    "\n",
    "    pred = DotPredictor()\n",
    "\n",
    "    # calculate the score of each user\n",
    "    scores = []\n",
    "    for i, score in enumerate(pred(user_g.edge_index, h)):\n",
    "        rel=1 if ((user_id,i) in test_pos_edges) else 0\n",
    "        scores.append((i, score,rel))\n",
    "\n",
    "    # produce final ranked list\n",
    "    scores.sort(key=lambda x: -x[1])\n",
    "\n",
    "    # display results\n",
    "    \n",
    "    if (k !=0):\n",
    "        print(f\"List of 5 suggested friends for user {user_id}:\")\n",
    "    for i in range(k):\n",
    "        print(f'- User {scores[i][0]}, score = {scores[i][1]}, rel = {scores[i][2]}')\n",
    "    return scores[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bd06a00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rec(h, user_id=0, k=10):\n",
    "    user_friends = set()\n",
    "    user_neg_u, user_neg_v = [], []\n",
    "\n",
    "    for n1, n2 in data.edge_index.t().tolist():   # get all friends of user_id\n",
    "        if int(n1) == user_id:\n",
    "            user_friends.add(int(n2))\n",
    "        if int(n2) == user_id:\n",
    "            user_friends.add(int(n1))\n",
    "\n",
    "    num_nodes=data.x.shape[0]\n",
    "    for i in range(num_nodes):  # generate \"negative edges\" for user_id\n",
    "        if i != user_id and i not in user_friends:\n",
    "            user_neg_u.append(user_id)\n",
    "            user_neg_v.append(i)\n",
    "\n",
    "    user_g = Data(x=data.x,edge_index=torch.tensor([user_neg_u, user_neg_v] ))\n",
    "\n",
    "    pred = DotPredictor()\n",
    "\n",
    "    # calculate the score of each user\n",
    "    scores = []\n",
    "    for i, score in enumerate(pred(user_g.edge_index, h)):\n",
    "        rel=1 if ((user_id,i) in test_pos_edges) else 0\n",
    "        scores.append((i, score,rel))\n",
    "\n",
    "    # produce final ranked list\n",
    "    scores.sort(key=lambda x: -x[1])\n",
    "    rel=[x[2] for x in scores]\n",
    "    # display results\n",
    "    \n",
    "    return rel[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68a50391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(a,k=50):\n",
    "    top_k = a[:k]\n",
    "    hits=pd.DataFrame(0,index=[0],columns=top_k.columns)\n",
    "    recipr=pd.DataFrame(0,index=[0],columns=top_k.columns)\n",
    "    dcg=pd.DataFrame(0,index=[0],columns=top_k.columns)\n",
    "    idcg=pd.DataFrame(0,index=[0],columns=top_k.columns)\n",
    "    for i in list(top_k.columns):\n",
    "        hits[i]=(top_k[i].sum()/k)\n",
    "        recipr[i]=top_k.index[top_k[i] == 1].min()\n",
    "        dcg[i]=0\n",
    "        idcg[i]=0\n",
    "        top_sort=top_k[i]\n",
    "        top_sort=top_sort.sort_values(ascending=False)\n",
    "        for j in range(0,k):\n",
    "            dcg[i]+=top_k[i].iloc[j]/np.log2(j+1+1)\n",
    "            idcg[i]+=top_sort.iloc[j]/np.log2(j+1+1)\n",
    "\n",
    "    recipr=recipr.replace(np.nan,-1) +1\n",
    "    recipr = recipr.loc[:, (recipr != 0).any()]\n",
    "    ndcg=dcg/idcg\n",
    "    ndcg=ndcg.T.replace(np.nan,0)\n",
    "\n",
    "    ndcg_k=np.mean(ndcg)\n",
    "    mrr=(1/recipr.T).sum()[0]/len(a.columns)\n",
    "    hits_at_k=np.mean(hits)\n",
    "\n",
    "    print('Hits@'+str(k)+':', hits_at_k)\n",
    "    print('NDCG@'+str(k)+':', ndcg_k)\n",
    "    print('MRR:', mrr)\n",
    "    \n",
    "    return hits_at_k, ndcg_k, mrr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380dd537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f717a06-f73a-4f7f-84f5-a45294ef276f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "index 1702611 is out of bounds for dimension 0 with size 81306",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1900010/1544061088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GCN\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1900010/2627030988.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(model_name, hidden_size, epoch)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'GCN'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'TruncatedSVD'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get node embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1900010/466478891.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, edge_index, x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_edge_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                     edge_index, edge_weight = gcn_norm(  # yapf: disable\n\u001b[0m\u001b[1;32m    223\u001b[0m                         \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                         self.improved, self.add_self_loops, self.flow, x.dtype)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py\u001b[0m in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mflow\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'source_to_target'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mdeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mdeg_inv_sqrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mdeg_inv_sqrt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked_fill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeg_inv_sqrt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch_geometric/utils/scatter.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sum'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'add'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_add_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index 1702611 is out of bounds for dimension 0 with size 81306"
     ]
    }
   ],
   "source": [
    "h = pipeline(\"GCN\",epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba82b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_users = np.unique(np.array(test_pos_edges).flatten())\n",
    "rankings={}\n",
    "with tqdm(total=len(target_users), position=0, leave=True) as pbar:\n",
    "    for user in target_users:         \n",
    "        rankings[user]=calc_rec(h,user)\n",
    "        pbar.update()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9fbb511e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 14955456 is out of bounds for dimension 0 with size 81306",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1900010/2268779382.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mh2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TruncatedSVD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1900010/2627030988.py\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(model_name, hidden_size, epoch)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mpos_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mneg_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_neg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1900010/2348939340.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, edge_index, h)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# Get the embeddings of the source nodes and destination nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0msource_node_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mdestination_node_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0medge_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 14955456 is out of bounds for dimension 0 with size 81306"
     ]
    }
   ],
   "source": [
    "h2 = pipeline(\"TruncatedSVD\",epoch=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778da106",
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings2={}\n",
    "with tqdm(total=len(target_users), position=0, leave=True) as pbar:\n",
    "    for user in target_users:              \n",
    "        rankings2[user]=calc_rec(h2,user)\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e61450",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=pd.DataFrame(rankings)\n",
    "#all users\n",
    "metrics(a,k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2138068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(a,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e524b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_csv('dot_product_gcn_all'+'_'+'twitter'+'.csv', index=False) \n",
    "a.to_pickle('dot_product_gcn_all'+'_'+'twitter'+'.pkl')\n",
    "torch.save(h, 'node_embeddings_gcn'+'_'+'twitter'+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c740d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2=pd.DataFrame(rankings2)\n",
    "#all users\n",
    "metrics(a2,k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b3fe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics(a2,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e79eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2.to_csv('dot_product_TruncatedSVD_all'+'_'+'twitter'+'.csv', index=False) \n",
    "a2.to_pickle('dot_product_TruncatedSVD_all'+'_'+'twitter'+'.pkl')\n",
    "torch.save(h2, 'node_embeddings_TruncatedSVD'+'_'+'twitter'+'.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ecc60",
   "metadata": {},
   "source": [
    "|model|AUC|hits@5|NDCG@5| MRR  |hits@50|NDCG@50|MRR|\n",
    "|-----|---|---|------|------|-------|-------|---|\n",
    "| GCN |0.9596|0.0096|0.0273|0.0212| 0.0095|0.0868 |0.036  |\n",
    "|tSVD |0.8556|0.0043|0.0126|0.0097| 0.0048|0.0448 |0.017 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7197f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load if unloaded and get recommendations\n",
    "if 'h' not in globals() or 'h' not in locals():\n",
    "    h=torch.load('node_embeddings_gcn.pt')\n",
    "if 'a' not in globals() or 'a' not in locals():\n",
    "    a = pd.read_pickle('dot_product_gcn_all.pkl')\n",
    "if 'h2' not in globals() or 'h2' not in locals():\n",
    "    h=torch.load('node_embeddings_TruncatedSVD.pt')\n",
    "if 'a2' not in globals() or 'a2' not in locals():\n",
    "    a = pd.read_pickle('dot_product_TruncatedSVD_all.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cfc016",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc=generate_rec(h, user_id=40, k=5) #k only for visualization here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6774d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc2=generate_rec(h2, user_id=40, k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
